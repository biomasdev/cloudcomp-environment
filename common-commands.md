## Task execution and common commands

### HDFS

The following commands are frequent in handling in HDFS:

```{bash}
hdfs dfs -ls /
hdfs dfs -mkdir /user/login/folder
hdfs dfs -put ~/content /user/login
hdfs dfs -get /user/login/file ~
```

## Spark

The cluster uses Apache Yarn for resource scaling for Spark (for more information, see the official documentation). Thus, application submissions to the cluster must follow the following pattern:

- for interactive terminals (spark-shell), use as master yarn parameter:
    - Using scala: 

    `$ spark-shell --master yarn --deploy-mode client --num-executors <num> --executor-memory <mem> --executor-cores <cores>`
    - Using Python:

    `$ pyspark --master yarn --deploy-mode client --num-executors <num> --executor-memory <mem> --executor-cores <cores>`

It is important that these modes are used with caution, you can easily exhaust cluster resources by keeping a shell open indefinitely. For executions of an already implemented application, give preference to spark-submit:

`$ spark-submit --master yarn --deploy-mode <cluster or client> --num-executors <num> --executor-memory <mem> --executor-cores <cores> --class <classe-do-jar> <jar/py> <paramêtros>`

This way, in cluster mode, even your program's driver will be scheduled on some slave in the cluster. This relieves the master to serve multiple users.


## Jupyter

The cluster has Jupyter notebook installed for those who want to program in PySpark/Python. To use it, we suggest the following steps:

1. Start the jupyter service on the cluster, choose a free port:

`$ jupyter notebook --no-browser --port=<porta>`


2. Create an ssh tunnel on your machine to access your computer's Jupyter service:

`$ ssh <login>@vcm-23691.vm.duke.edu -L <porta>:vcm-23691.vm.duke.edu:<porta>`

3. Access the url provided by your computer's jupyter log.

4. Note: For Pyspark runs on Jupyter, you need to create the SparkSession dynamically ([doc](https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession)). Remember that you must configure the resources (memory, colors, master node address, etc) to be used in SparkContext/SparkSession, otherwise, you will be running Spark applications locally (on compute1 machine) and may overload the system;


## Yarn and Spark Run Monitoring

By default, Yarn provides a graphical interface (UI) for monitoring and controlling applications and resources on port 8088. To access the Yarn UI (on port 8088) it is necessary to tunnel ssh, using a command similar to:

`$ ssh login@vcm-23691.vm.duke.edu -fNT -L 8088:vcm-23691.vm.duke.edu:8088`


On this page, you can see the status of the clusters, as well as the amount of resources (CPU/RAM) available. In addition, when a Spark application is running, the user can access the logs generated by Spark, available in the Spark UI (http://localhost:{4040,4041,4042, …}). The exact port will appear at the start of execution or can be obtained from the Yarn UI. Once you have this correct address, make a new tunnel to that port.


## Kubernetes

Each user will have a Kubernetes namespace with the same login name. The user account allows to create, view, execute or delete resources in their namespaces.

## Redis

In Project 3, users will need read data from Redis (https://redis.io/). The service is running in vcm-23691 (port 6379). Students can access data by using a python [module](https://pypi.org/project/redis/), using a Spark integration or by a CLI interface (using the command `redis-cli`).


## ArgoCD

In Project 2, users will need to interact with ArgoCD. The framework supports a web UI interface (at port 8081) and CLI interface [doc](https://argo-cd.readthedocs.io/en/release-1.8/user-guide/commands/argocd/).
