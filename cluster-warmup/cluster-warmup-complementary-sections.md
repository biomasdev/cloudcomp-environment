## Hadoop MapReduce

In this example we will review and execute a WordCount program
implemented in the MapReduce paradigm. Study the code in
`WordCount.java`, it has extensive comments explaning how it works.

### Compiling the Code

After reviwing the code, we will create a directory to contain our
compiled classes.

```{bash}
mkdir classes
```

To compile the `WordCount.java` program, we need to provide Java with
the location where it can find the Hadoop libraries. We do this by
setting and passing `CLASSPATH` as a parameter:

```{bash}
export HADOOP_CP=$(hadoop classpath)
javac -cp $HADOOP_CP -d classes WordCount.java
jar -cvf wordcount.jar -C classes/ .
```

These commands should create a `wordcount.jar` file in the current
directory, which we will submit to Hadoop.

### Running WordCount

We provide two files as input for the WordCount example. These files
contain random text and are stored in `hdfs:/datasets/lorem/`. WordCount
Map instances will process these files from HDFS.

We can launch our code submitting it to Hadoop. We pass a JAR file
(`wordcount.jar`, generated in the previous step) and a class containing
the `main` method (`wordcount.WordCount`). We also pass two directories
as parameters (`<input>` and `<output>`); all files in `<input>` will be
read and processed by Map instances, and `<output>` will be written to
by Reduce instances.

```{bash}
hadoop jar wordcount.jar wordcount.WordCount <input> <output>
```

Where `<input>` should be set to the directories containing the random
files used as input (without the `hdfs://` prefix), and `<output>`
should point to a directory in the user's HDFS workspace. Note that
`<output>` is generated by WordCount and must not exist. Here is an
example:

```{bash}
hadoop jar wordcount.jar wordcount.WordCount \
                /datasets/lorem /user/cunha/wordcount
```

### Inspecting Results

We can verify the results by inspecting the files under the `<output>`
directory:

```{bash}
hdfs dfs -ls <output>
```

Use `hdfs dfs -cat <output>/<part>` to see the output. The output should
contain one word and the number of occurrences of that word per line.

## Spark

<!-- Bits removed on S3 2023 -->

The cluster uses [Apache Yarn][apache-yarn] for resource management and
job scheduling for Spark. Thus, interactive shells and job submissions
to the cluster must pass `--master yarn` as a parameter.

```{bash}
$ pyspark --master yarn --deploy-mode client \
        --num-executors 2 --executor-cores 2 \
        --executor-memory 1024M
(...)
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0
      /_/

Using Python version 3.8.10 (default, Nov 26 2021 20:14:08)
Spark context Web UI available at http://hostname:4040
Spark context available as 'sc' (master = yarn, app id = application_1641586816957_0002).
SparkSession available as 'spark'.
```

```{bash}
spark-submit --master yarn \
        --deploy-mode cluster \
        --num-executors 2 \
        --executor-cores 2 \
        --executor-memory 1024M \
        spark-warmup.py
```

```{python}
from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .master("yarn") \
    .appName("HelloLines") \
    .getOrCreate()
sc = spark.sparkContext
rdd = sc.textFile("hdfs:/user/cunha/hello.txt")
lines = rdd.count()
outrdd = sc.parallelize([lines])
# The following will fail if the output directory exists:
outrdd.saveAsTextFile("hdfs:/user/cunha/hello-linecount-submit")
sc.stop()
```

Most of the work in a Spark application is performed by executors,
including, for example, the loading of data and parallel processing
performed by calling Spark libraries. Executors always run on cluster
nodes. The [`--deploy-mode`][spark-doc-deploy-mode] parameter specifies
where the application code that you wrote, also called the application
*driver*, will run. If `client`, then the driver will run on the local
machine (the main VM); if `cluster`, then the driver will run on a
cluster node just like executors. Using `--deploy-mode cluster` for
long-running jobs frees up resources on the main VM for other
interactive sessions.

As Jupyter also runs outside the PySpark shell, we need to create the
SparkSession and get the SparkContext manually, as we did in
`spark-warmup.py` above. As Jupyter also does not get the command-line
parameters we passed to `pyspark` and `spark-submit`, you must configure
the resources (memory, cores, and executors) when initializing the
SparkSession, otherwise you will be running Spark applications locally,
that is, on the main VM, and may overload it. The following will create
a SparkSession with the same parameters used in `spark-submit` above:

## Yarn and Spark Run Monitoring

By default, Yarn provides a Web user interface (UI) for monitoring and
controlling applications and resources on port 8088. To access the Yarn
UI it is necessary to tunnel SSH, similar to how we accessed Jupyter:

```{bash}
ssh -fNT -L 8088:200.19.156.22:8088 \
        <username>@200.19.156.22 -p 4422
```

You can then access the page by navigating to `http://localhost:8088` on
your browser. On the page we can see the status of the cluster, as well
as the amount of resources (CPU/RAM) available.

Additionally, when a Spark application is running, the user can access
the logs generated by Spark, available in the Spark UI. Spark prints the
port number the UI is running on once it starts execution. You can make
another SSH tunnel to access the Spark UI. The last line below shows the
port number you should SSH tunnel into:

```{text}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0
      /_/

Using Python version 3.8.10 (default, Nov 26 2021 20:14:08)
Spark context Web UI available at http://localhost:4040
```

## Kubernetes

Each user will have a Kubernetes namespace with the same login name. The user account allows to create, view, execute or delete resources in their namespaces.

## Redis

In Project 3, users will need read data from [Redis](https://redis.io/). The service is running in vcm-23691 (port 6379). Students can access data by using a python [module](https://pypi.org/project/redis/), using a Spark integration or by a CLI interface (using the command `redis-cli`).

## ArgoCD

In Project 2, users will need to interact with ArgoCD. The framework supports a web UI interface (at port 8081) and CLI interface [doc](https://argo-cd.readthedocs.io/en/release-1.8/user-guide/commands/argocd/).
